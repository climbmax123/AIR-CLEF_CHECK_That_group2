{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural NLP Representation Learning Approach\n",
    "\n",
    "### CLEF 2025 - CheckThat! Lab  - Task 4 Scientific Web Discourse - Subtask 4b (Scientific Claim Source Retrieval)\n",
    "\n",
    "This notebook implements an improved neural approach using sentence transformers with:\n",
    "- Enhanced text preprocessing for scientific content\n",
    "- Multi-query retrieval with domain-specific augmentation\n",
    "- Semantic term matching boosts\n",
    "\n",
    "This remains a pure neural representation learning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.1.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.6.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (4.52.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scipy in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (0.32.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emirhan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Emirhan\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Importing data and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emirhan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a) Import the collection set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_COLLECTION_DATA = 'subtask4b_collection_data.pkl' #MODIFY PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7718 entries, 162 to 1056448\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   cord_uid          7718 non-null   object        \n",
      " 1   source_x          7718 non-null   object        \n",
      " 2   title             7718 non-null   object        \n",
      " 3   doi               7677 non-null   object        \n",
      " 4   pmcid             4959 non-null   object        \n",
      " 5   pubmed_id         6233 non-null   object        \n",
      " 6   license           7718 non-null   object        \n",
      " 7   abstract          7718 non-null   object        \n",
      " 8   publish_time      7715 non-null   object        \n",
      " 9   authors           7674 non-null   object        \n",
      " 10  journal           6668 non-null   object        \n",
      " 11  mag_id            0 non-null      float64       \n",
      " 12  who_covidence_id  528 non-null    object        \n",
      " 13  arxiv_id          20 non-null     object        \n",
      " 14  label             7718 non-null   object        \n",
      " 15  time              7715 non-null   datetime64[ns]\n",
      " 16  timet             7718 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(14)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_collection.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>mag_id</th>\n",
       "      <th>who_covidence_id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Professional and Home-Made Face Masks Reduce E...</td>\n",
       "      <td>10.1371/journal.pone.0002618</td>\n",
       "      <td>PMC2440799</td>\n",
       "      <td>18612429</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>BACKGROUND: Governments are preparing for a po...</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>van der Sande, Marianne; Teunis, Peter; Sabel,...</td>\n",
       "      <td>PLoS One</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>1215561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>PMC</td>\n",
       "      <td>The Failure of R (0)</td>\n",
       "      <td>10.1155/2011/527610</td>\n",
       "      <td>PMC3157160</td>\n",
       "      <td>21860658</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The basic reproductive ratio, R (0), is one of...</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>Li, Jing; Blakeley, Daniel; Smith?, Robert J.</td>\n",
       "      <td>Comput Math Methods Med</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>1313452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Pulmonary sequelae in a patient recovered from...</td>\n",
       "      <td>10.4103/0970-2113.99118</td>\n",
       "      <td>PMC3424870</td>\n",
       "      <td>22919170</td>\n",
       "      <td>cc-by-nc-sa</td>\n",
       "      <td>The pandemic of swine flu (H1N1) influenza spr...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Singh, Virendra; Sharma, Bharat Bhushan; Patel...</td>\n",
       "      <td>Lung India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>1325376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>PMC</td>\n",
       "      <td>What was the primary mode of smallpox transmis...</td>\n",
       "      <td>10.3389/fcimb.2012.00150</td>\n",
       "      <td>PMC3509329</td>\n",
       "      <td>23226686</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The mode of infection transmission has profoun...</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>Milton, Donald K.</td>\n",
       "      <td>Front Cell Infect Microbiol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>1354147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Lessons from the History of Quarantine, from P...</td>\n",
       "      <td>10.3201/eid1902.120312</td>\n",
       "      <td>PMC3559034</td>\n",
       "      <td>23343512</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>In the new millennium, the centuries-old strat...</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>Tognotti, Eugenia</td>\n",
       "      <td>Emerg Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>1359849600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cord_uid source_x                                              title  \\\n",
       "162   umvrwgaw      PMC  Professional and Home-Made Face Masks Reduce E...   \n",
       "611   spiud6ok      PMC                               The Failure of R (0)   \n",
       "918   aclzp3iy      PMC  Pulmonary sequelae in a patient recovered from...   \n",
       "993   ycxyn2a2      PMC  What was the primary mode of smallpox transmis...   \n",
       "1053  zxe95qy9      PMC  Lessons from the History of Quarantine, from P...   \n",
       "\n",
       "                               doi       pmcid pubmed_id      license  \\\n",
       "162   10.1371/journal.pone.0002618  PMC2440799  18612429        cc-by   \n",
       "611            10.1155/2011/527610  PMC3157160  21860658        cc-by   \n",
       "918        10.4103/0970-2113.99118  PMC3424870  22919170  cc-by-nc-sa   \n",
       "993       10.3389/fcimb.2012.00150  PMC3509329  23226686        cc-by   \n",
       "1053        10.3201/eid1902.120312  PMC3559034  23343512        no-cc   \n",
       "\n",
       "                                               abstract publish_time  \\\n",
       "162   BACKGROUND: Governments are preparing for a po...   2008-07-09   \n",
       "611   The basic reproductive ratio, R (0), is one of...   2011-08-16   \n",
       "918   The pandemic of swine flu (H1N1) influenza spr...         2012   \n",
       "993   The mode of infection transmission has profoun...   2012-11-29   \n",
       "1053  In the new millennium, the centuries-old strat...   2013-02-03   \n",
       "\n",
       "                                                authors  \\\n",
       "162   van der Sande, Marianne; Teunis, Peter; Sabel,...   \n",
       "611       Li, Jing; Blakeley, Daniel; Smith?, Robert J.   \n",
       "918   Singh, Virendra; Sharma, Bharat Bhushan; Patel...   \n",
       "993                                   Milton, Donald K.   \n",
       "1053                                  Tognotti, Eugenia   \n",
       "\n",
       "                          journal  mag_id who_covidence_id arxiv_id     label  \\\n",
       "162                      PLoS One     NaN              NaN      NaN  umvrwgaw   \n",
       "611       Comput Math Methods Med     NaN              NaN      NaN  spiud6ok   \n",
       "918                    Lung India     NaN              NaN      NaN  aclzp3iy   \n",
       "993   Front Cell Infect Microbiol     NaN              NaN      NaN  ycxyn2a2   \n",
       "1053             Emerg Infect Dis     NaN              NaN      NaN  zxe95qy9   \n",
       "\n",
       "           time       timet  \n",
       "162  2008-07-09  1215561600  \n",
       "611  2011-08-16  1313452800  \n",
       "918  2012-01-01  1325376000  \n",
       "993  2012-11-29  1354147200  \n",
       "1053 2013-02-03  1359849600  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b) Import the query set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_QUERY_TRAIN_DATA = 'subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = 'subtask4b_query_tweets_dev.tsv' #MODIFY PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep='\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oral care in rehabilitation medicine: oral vul...</td>\n",
       "      <td>htlvpvz5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this study isn't receiving sufficient attentio...</td>\n",
       "      <td>4kfl29ul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>thanks, xi jinping. a reminder that this study...</td>\n",
       "      <td>jtwb17u8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Taiwan - a population of 23 million has had ju...</td>\n",
       "      <td>0w9k8iy1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Obtaining a diagnosis of autism in lower incom...</td>\n",
       "      <td>tiqksd69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid\n",
       "0        0  Oral care in rehabilitation medicine: oral vul...  htlvpvz5\n",
       "1        1  this study isn't receiving sufficient attentio...  4kfl29ul\n",
       "2        2  thanks, xi jinping. a reminder that this study...  jtwb17u8\n",
       "3        3  Taiwan - a population of 23 million has had ju...  0w9k8iy1\n",
       "4        4  Obtaining a diagnosis of autism in lower incom...  tiqksd69"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>covid recovery: this study from the usa reveal...</td>\n",
       "      <td>3qvh482o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>\"Among 139 clients exposed to two symptomatic ...</td>\n",
       "      <td>r58aohnu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>I recall early on reading that researchers who...</td>\n",
       "      <td>sts48u9i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>You know you're credible when NIH website has ...</td>\n",
       "      <td>3sr2exq9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>Resistance to antifungal medications is a grow...</td>\n",
       "      <td>ybwwmyqy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid\n",
       "0       16  covid recovery: this study from the usa reveal...  3qvh482o\n",
       "1       69  \"Among 139 clients exposed to two symptomatic ...  r58aohnu\n",
       "2       73  I recall early on reading that researchers who...  sts48u9i\n",
       "3       93  You know you're credible when NIH website has ...  3sr2exq9\n",
       "4       96  Resistance to antifungal medications is a grow...  ybwwmyqy"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Text preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_text(text):\n",
    "    \"\"\"Clean tweet text while preserving scientific information\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    text = re.sub(r'&amp;', 'and', text)\n",
    "    text = re.sub(r'&lt;', '<', text)\n",
    "    text = re.sub(r'&gt;', '>', text)\n",
    "    \n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    text = re.sub(r'#covid19', 'COVID-19', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'#sarscov2', 'SARS-CoV-2', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'#(covid|coronavirus)', 'COVID-19', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    text = re.sub(r'\\bcovid-?19\\b', 'COVID-19', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bsars-?cov-?2\\b', 'SARS-CoV-2', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bcovid\\b(?![\\d-])', 'COVID-19', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    text = re.sub(r'\\bnih\\b', 'NIH', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bicu\\b', 'ICU', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bppe\\b', 'PPE', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\busa\\b', 'USA', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    text = re.sub(r'\\bp\\s*[<>=]\\s*0\\.(\\d+)', r'p-value 0.\\1', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(\\d+)%\\s*ci\\b', r'\\1% confidence interval', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    text = re.sub(r'[💃🚨▶️👍📈📊🔥✅❌🎯🧵👇🏻🔴☑️⬇️➡️]+', '', text)\n",
    "    text = re.sub(r'[\\\"\\\"\\\"]', '\"', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_scientific_text(text):\n",
    "    \"\"\"Minimal cleaning for scientific text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_enhanced_document_text(row):\n",
    "    \"\"\"Create structured document representation\"\"\"\n",
    "    title = clean_scientific_text(row['title'])\n",
    "    abstract = clean_scientific_text(row['abstract'])\n",
    "    authors = str(row['authors']) if not pd.isna(row['authors']) else \"\"\n",
    "    journal = str(row['journal']) if not pd.isna(row['journal']) else \"\"\n",
    "    \n",
    "    enhanced_text = f\"Title: {title}. Abstract: {abstract}\"\n",
    "    \n",
    "    if authors:\n",
    "        enhanced_text += f\" Authors: {authors}\"\n",
    "    if journal:\n",
    "        enhanced_text += f\" Journal: {journal}\"\n",
    "    \n",
    "    return enhanced_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Load sentence transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Prepare document representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced representations for all documents\n",
    "df_collection['enhanced_text'] = df_collection.apply(create_enhanced_document_text, axis=1)\n",
    "\n",
    "# Prepare corpus and IDs\n",
    "corpus = df_collection['enhanced_text'].tolist()\n",
    "cord_uids = df_collection['cord_uid'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Encode all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 242/242 [01:25<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode all documents\n",
    "doc_embeddings = model.encode(\n",
    "    corpus,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    batch_size=32,\n",
    "    normalize_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Advanced retrieval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmented_queries(tweet_text):\n",
    "    \"\"\"Create multiple query variations for better retrieval\"\"\"\n",
    "    base_query = clean_tweet_text(tweet_text)\n",
    "    queries = [base_query]\n",
    "    base_lower = base_query.lower()\n",
    "    \n",
    "    # Add scientific context if study-related terms present\n",
    "    if any(term in base_lower for term in ['study', 'research', 'trial', 'analysis', 'findings']):\n",
    "        queries.append(f\"scientific research {base_query}\")\n",
    "    \n",
    "    # Add COVID context if relevant\n",
    "    if any(term in base_lower for term in ['covid', 'coronavirus', 'pandemic', 'vaccine', 'mask']):\n",
    "        queries.append(f\"COVID-19 pandemic study {base_query}\")\n",
    "    \n",
    "    # Add statistical context if numbers present\n",
    "    if re.search(r'\\d+%|\\bp-value|\\bconfidence interval|\\bodds ratio|\\brisk', base_query, re.IGNORECASE):\n",
    "        queries.append(f\"statistical research findings {base_query}\")\n",
    "    \n",
    "    return queries\n",
    "\n",
    "\n",
    "def precompute_doc_features():\n",
    "    \"\"\"Pre-compute boolean arrays for fast boosting\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    corpus_lower = [doc.lower() for doc in corpus]\n",
    "    \n",
    "    features['covid'] = np.array([\n",
    "        any(term in doc for term in ['covid', 'coronavirus']) \n",
    "        for doc in corpus_lower\n",
    "    ], dtype=float)\n",
    "    \n",
    "    features['stats'] = np.array([\n",
    "        bool(re.search(r'\\d+%|\\bp-value', doc)) \n",
    "        for doc in corpus_lower\n",
    "    ], dtype=float)\n",
    "    \n",
    "    features['study'] = np.array([\n",
    "        any(term in doc for term in ['study', 'trial', 'research']) \n",
    "        for doc in corpus_lower\n",
    "    ], dtype=float)\n",
    "    \n",
    "    features['medical'] = np.array([\n",
    "        any(term in doc for term in ['vaccine', 'mask', 'treatment']) \n",
    "        for doc in corpus_lower\n",
    "    ], dtype=float)\n",
    "    \n",
    "    return features\n",
    "\n",
    "doc_features = precompute_doc_features()\n",
    "\n",
    "def compute_semantic_boost(query_text, doc_features):\n",
    "    \"\"\"Vectorized semantic boosting\"\"\"\n",
    "    query_lower = query_text.lower()\n",
    "    boosts = np.zeros(len(doc_features['covid']))\n",
    "    \n",
    "    if any(term in query_lower for term in ['covid', 'coronavirus']):\n",
    "        boosts += 0.05 * doc_features['covid']\n",
    "    \n",
    "    if re.search(r'\\d+%|\\bp-value', query_lower):\n",
    "        boosts += 0.06 * doc_features['stats']\n",
    "    \n",
    "    if any(term in query_lower for term in ['study', 'trial', 'research']):\n",
    "        boosts += 0.03 * doc_features['study']\n",
    "    \n",
    "    if any(term in query_lower for term in ['vaccine', 'mask', 'treatment']):\n",
    "        boosts += 0.04 * doc_features['medical']\n",
    "    \n",
    "    return boosts\n",
    "\n",
    "def retrieve_papers(query_text, k=5):\n",
    "    \"\"\"Neural retrieval using multi-query and semantic boosting\"\"\"\n",
    "    queries = create_augmented_queries(query_text)\n",
    "    \n",
    "    all_similarities = []\n",
    "    for query in queries:\n",
    "        query_embedding = model.encode([query], \n",
    "                                     convert_to_numpy=True, \n",
    "                                     normalize_embeddings=True)\n",
    "        base_similarities = cosine_similarity(query_embedding, doc_embeddings).flatten()\n",
    "        \n",
    "        boosts = compute_semantic_boost(query, doc_features)\n",
    "        boosted_similarities = base_similarities + boosts\n",
    "        all_similarities.append(boosted_similarities)\n",
    "    \n",
    "    if len(all_similarities) == 1:\n",
    "        fused_scores = all_similarities[0]\n",
    "    else:\n",
    "        weights = [0.6] + [0.4 / (len(all_similarities) - 1)] * (len(all_similarities) - 1)\n",
    "        fused_scores = np.average(all_similarities, axis=0, weights=weights)\n",
    "    \n",
    "    top_indices = np.argsort(fused_scores)[::-1][:k]\n",
    "    return [cord_uids[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Running the improved neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve topk candidates using the improved neural model\n",
    "df_query_train['improved_neural_topk'] = df_query_train['tweet_text'].apply(lambda x: retrieve_papers(x))\n",
    "df_query_dev['improved_neural_topk'] = df_query_dev['tweet_text'].apply(lambda x: retrieve_papers(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Evaluating the improved neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the train set: {1: np.float64(0.4220804481444021), 5: np.float64(0.4897196504058715), 10: np.float64(0.4897196504058715)}\n",
      "Results on the dev set: {1: np.float64(0.4392857142857143), 5: np.float64(0.5115000000000001), 10: np.float64(0.5115000000000001)}\n"
     ]
    }
   ],
   "source": [
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'improved_neural_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'improved_neural_topk')\n",
    "# Printed MRR@k results in the following format: {k: MRR@k}\n",
    "print(f\"Results on the train set: {results_train}\")\n",
    "print(f\"Results on the dev set: {results_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Exporting results to prepare the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_dev['preds'] = df_query_dev['improved_neural_topk'].apply(lambda x: x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_dev[['post_id', 'preds']].to_csv('predictions_improved_neural.tsv', index=None, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
