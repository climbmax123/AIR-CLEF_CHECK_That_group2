{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural NLP Representation Learning Approach\n",
    "\n",
    "### CLEF 2025 - CheckThat! Lab  - Task 4 Scientific Web Discourse - Subtask 4b (Scientific Claim Source Retrieval)\n",
    "\n",
    "This notebook implements an improved neural approach using sentence transformers with:\n",
    "- Enhanced text preprocessing for scientific content\n",
    "- Multi-query retrieval with domain-specific augmentation\n",
    "- Semantic term matching boosts\n",
    "\n",
    "This remains a pure neural representation learning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Importing data and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a) Import the collection set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_COLLECTION_DATA = 'subtask4b_collection_data.pkl' #MODIFY PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collection.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b) Import the query set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_QUERY_TRAIN_DATA = 'subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = 'subtask4b_query_tweets_dev.tsv' #MODIFY PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep='\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Text preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_text(text):\n",
    "    \"\"\"Clean tweet text while preserving scientific information\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    text = re.sub(r'&amp;', 'and', text)\n",
    "    text = re.sub(r'&lt;', '<', text)\n",
    "    text = re.sub(r'&gt;', '>', text)\n",
    "    \n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    text = re.sub(r'#covid19', 'COVID-19', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'#sarscov2', 'SARS-CoV-2', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'#(covid|coronavirus)', 'COVID-19', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    text = re.sub(r'\\bcovid-?19\\b', 'COVID-19', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bsars-?cov-?2\\b', 'SARS-CoV-2', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bcovid\\b(?![\\d-])', 'COVID-19', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    text = re.sub(r'\\bnih\\b', 'NIH', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bicu\\b', 'ICU', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bppe\\b', 'PPE', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\busa\\b', 'USA', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    text = re.sub(r'\\bp\\s*[<>=]\\s*0\\.(\\d+)', r'p-value 0.\\1', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(\\d+)%\\s*ci\\b', r'\\1% confidence interval', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    text = re.sub(r'[üíÉüö®‚ñ∂Ô∏èüëçüìàüìäüî•‚úÖ‚ùåüéØüßµüëáüèªüî¥‚òëÔ∏è‚¨áÔ∏è‚û°Ô∏è]+', '', text)\n",
    "    text = re.sub(r'[\\\"\\\"\\\"]', '\"', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_scientific_text(text):\n",
    "    \"\"\"Minimal cleaning for scientific text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_enhanced_document_text(row):\n",
    "    \"\"\"Create structured document representation\"\"\"\n",
    "    title = clean_scientific_text(row['title'])\n",
    "    abstract = clean_scientific_text(row['abstract'])\n",
    "    authors = str(row['authors']) if not pd.isna(row['authors']) else \"\"\n",
    "    journal = str(row['journal']) if not pd.isna(row['journal']) else \"\"\n",
    "    \n",
    "    enhanced_text = f\"Title: {title}. Abstract: {abstract}\"\n",
    "    \n",
    "    if authors:\n",
    "        enhanced_text += f\" Authors: {authors}\"\n",
    "    if journal:\n",
    "        enhanced_text += f\" Journal: {journal}\"\n",
    "    \n",
    "    return enhanced_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Load sentence transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Prepare document representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced representations for all documents\n",
    "df_collection['enhanced_text'] = df_collection.apply(create_enhanced_document_text, axis=1)\n",
    "\n",
    "# Prepare corpus and IDs\n",
    "corpus = df_collection['enhanced_text'].tolist()\n",
    "cord_uids = df_collection['cord_uid'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Encode all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all documents\n",
    "doc_embeddings = model.encode(\n",
    "    corpus,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    batch_size=32,\n",
    "    normalize_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Advanced retrieval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmented_queries(tweet_text):\n",
    "    \"\"\"Create multiple query variations for better retrieval\"\"\"\n",
    "    base_query = clean_tweet_text(tweet_text)\n",
    "    queries = [base_query]\n",
    "    base_lower = base_query.lower()\n",
    "    \n",
    "    # Add scientific context if study-related terms present\n",
    "    if any(term in base_lower for term in ['study', 'research', 'trial', 'analysis', 'findings']):\n",
    "        queries.append(f\"scientific research {base_query}\")\n",
    "    \n",
    "    # Add COVID context if relevant\n",
    "    if any(term in base_lower for term in ['covid', 'coronavirus', 'pandemic', 'vaccine', 'mask']):\n",
    "        queries.append(f\"COVID-19 pandemic study {base_query}\")\n",
    "    \n",
    "    # Add statistical context if numbers present\n",
    "    if re.search(r'\\d+%|\\bp-value|\\bconfidence interval|\\bodds ratio|\\brisk', base_query, re.IGNORECASE):\n",
    "        queries.append(f\"statistical research findings {base_query}\")\n",
    "    \n",
    "    return queries\n",
    "\n",
    "def compute_semantic_boost(query_text, corpus_texts):\n",
    "    \"\"\"Compute semantic boost factors based on domain-specific term matching\"\"\"\n",
    "    query_lower = query_text.lower()\n",
    "    boosts = np.zeros(len(corpus_texts))\n",
    "    \n",
    "    # boosts\n",
    "    for i, doc_text in enumerate(corpus_texts):\n",
    "        doc_lower = doc_text.lower()\n",
    "        boost = 0.0\n",
    "        \n",
    "       \n",
    "        if re.search(r'\\d+%|\\bp-value', query_lower) and re.search(r'\\d+%|\\bp-value', doc_lower):\n",
    "            boost += 0.06\n",
    "        \n",
    "        if any(term in query_lower for term in ['covid', 'coronavirus']):\n",
    "            if any(term in doc_lower for term in ['covid', 'coronavirus']):\n",
    "                boost += 0.05\n",
    "        \n",
    "        if any(term in query_lower for term in ['study', 'trial', 'research']):\n",
    "            if any(term in doc_lower for term in ['study', 'trial', 'research']):\n",
    "                boost += 0.03\n",
    "\n",
    "        if any(term in query_lower for term in ['vaccine', 'mask', 'treatment']):\n",
    "            if any(term in doc_lower for term in ['vaccine', 'mask', 'treatment']):\n",
    "                boost += 0.04\n",
    "        \n",
    "        boosts[i] = boost\n",
    "    \n",
    "    return boosts\n",
    "\n",
    "def retrieve_papers_fast_boosting(query_text, k=5):\n",
    "    \"\"\"Fast neural retrieval using multi-query and semantic boosting\"\"\"\n",
    "    \n",
    "    queries = create_augmented_queries(query_text)\n",
    "    \n",
    "    all_similarities = []\n",
    "    for query in queries:\n",
    "        query_embedding = model.encode([query], \n",
    "                                     convert_to_numpy=True, \n",
    "                                     normalize_embeddings=True)\n",
    "        base_similarities = cosine_similarity(query_embedding, doc_embeddings).flatten()\n",
    "        \n",
    "        boosts = compute_semantic_boost(query, corpus)\n",
    "        boosted_similarities = base_similarities + boosts\n",
    "        all_similarities.append(boosted_similarities)\n",
    "    \n",
    "    if len(all_similarities) == 1:\n",
    "        fused_scores = all_similarities[0]\n",
    "    else:\n",
    "        weights = [0.6] + [0.4 / (len(all_similarities) - 1)] * (len(all_similarities) - 1)\n",
    "        fused_scores = np.average(all_similarities, axis=0, weights=weights)\n",
    "    \n",
    "    top_indices = np.argsort(fused_scores)[::-1][:k]\n",
    "    return [cord_uids[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Running the improved neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve topk candidates using the improved neural model\n",
    "df_query_train['improved_neural_topk'] = df_query_train['tweet_text'].apply(lambda x: retrieve_papers_fast_boosting(x))\n",
    "df_query_dev['improved_neural_topk'] = df_query_dev['tweet_text'].apply(lambda x: retrieve_papers_fast_boosting(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Evaluating the improved neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'improved_neural_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'improved_neural_topk')\n",
    "# Printed MRR@k results in the following format: {k: MRR@k}\n",
    "print(f\"Results on the train set: {results_train}\")\n",
    "print(f\"Results on the dev set: {results_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Exporting results to prepare the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_dev['preds'] = df_query_dev['improved_neural_topk'].apply(lambda x: x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_dev[['post_id', 'preds']].to_csv('predictions_improved_neural.tsv', index=None, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
